{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBN architecture\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbm1 = RBM(input_dim, hidden_dim)\n",
    "        self.rbm2 = RBM(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.rbm1(x))\n",
    "        x = torch.sigmoid(self.rbm2(x))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# RBM layer\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, visible_dim, hidden_dim):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(visible_dim, hidden_dim))\n",
    "        self.visible_bias = nn.Parameter(torch.randn(visible_dim))\n",
    "        self.hidden_bias = nn.Parameter(torch.randn(hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        p_hidden_given_visible = torch.sigmoid(torch.matmul(x, self.W) + self.hidden_bias)\n",
    "        sampled_hidden = torch.bernoulli(p_hidden_given_visible)\n",
    "        p_visible_given_hidden = torch.sigmoid(torch.matmul(sampled_hidden, self.W.t()) + self.visible_bias)\n",
    "        return p_visible_given_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j and retrieve knowledge graph vectors\n",
    "class Neo4jDataLoader:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def get_vectors(self):\n",
    "        with self._driver.session() as session:\n",
    "            # query = \"MATCH (node:Entity) RETURN node.vector AS vector\"\n",
    "            query = \"MATCH (n) RETURN n.Vector AS vector\"\n",
    "            result = session.run(query)\n",
    "            vectors = [record['vector'] for record in result]\n",
    "        return torch.tensor(vectors)\n",
    "neo4j_loader = Neo4jDataLoader(uri=\"neo4j://localhost:7687\", user=\"neo4j\", password=\"12345678\")\n",
    "data = neo4j_loader.get_vectors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"12345678\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"MATCH (n) RETURN n.label AS label\")\n",
    "    label = pd.DataFrame([record.values() for record in result], columns=result.keys())\n",
    "    \n",
    "# Extract the values from the 'label' column and convert to a one-dimensional list\n",
    "label_values = label['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Word2Vec model (you need to define this as you did before)\n",
    "sentences = [str(text).split() for text in label_values]\n",
    "model = Word2Vec(sentences, vector_size=1, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Function to get embeddings for a list of words\n",
    "def get_sentence_embedding(word_list):\n",
    "    word_vectors = [model.wv[word] for word in word_list if word in model.wv.key_to_index]\n",
    "    \n",
    "    if word_vectors:\n",
    "        sentence_embedding = sum(word_vectors)\n",
    "        return sentence_embedding\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'text_data': label_values}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the text_data column into lists of words and apply the function to each row\n",
    "df['text_data'] = df['text_data'].apply(lambda x: x.split() if x is not None else [])\n",
    "df['embeddings'] = df['text_data'].apply(lambda x: get_sentence_embedding(x) if x else None)\n",
    "\n",
    "print(df['embeddings'])\n",
    "\n",
    "# # Filter out rows where embeddings are not available\n",
    "# df = df.dropna(subset=['embeddings'])\n",
    "\n",
    "# Replace rows where embeddings are not available with a default value (e.g., zeros)\n",
    "default_embedding = np.zeros(1)  # Replace with your desired default value\n",
    "df['embeddings'] = df['embeddings'].apply(lambda x: x if x is not None else default_embedding)\n",
    "\n",
    "# Convert embeddings to a PyTorch tensor\n",
    "embeddings_tensor = torch.tensor(df['embeddings'].to_list())\n",
    "\n",
    "print(embeddings_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "def train_dbn():\n",
    "    # Initialize DBN and other hyperparameters\n",
    "    input_dim = 32  # Adjust based on the dimensionality of your knowledge graph vectors\n",
    "    hidden_dim = 32\n",
    "    output_dim = 2  # Adjust based on your task (e.g., classification)\n",
    "\n",
    "    dbn = DBN(input_dim, hidden_dim, output_dim)\n",
    "    optimizer = optim.Adam(dbn.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Connect to Neo4j and retrieve data\n",
    "    neo4j_loader = Neo4jDataLoader(uri=\"neo4j://localhost:7687\", user=\"neo4j\", password=\"12345678\")\n",
    "    data = neo4j_loader.get_vectors()\n",
    "\n",
    "    # Load labels for your data\n",
    "    labels = embeddings_tensor\n",
    "\n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    batch_size = 32  # Adjust based on your dataset size\n",
    "    dataset = TensorDataset(data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):  # Adjust the number of epochs\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = dbn(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dbn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
