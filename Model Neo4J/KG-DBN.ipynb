{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBN architecture\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbm1 = RBM(input_dim, hidden_dim)\n",
    "        self.rbm2 = RBM(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.rbm1(x))\n",
    "        x = torch.sigmoid(self.rbm2(x))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# RBM layer\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, visible_dim, hidden_dim):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(visible_dim, hidden_dim))\n",
    "        self.visible_bias = nn.Parameter(torch.randn(visible_dim))\n",
    "        self.hidden_bias = nn.Parameter(torch.randn(hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        p_hidden_given_visible = torch.sigmoid(torch.matmul(x, self.W) + self.hidden_bias)\n",
    "        sampled_hidden = torch.bernoulli(p_hidden_given_visible)\n",
    "        p_visible_given_hidden = torch.sigmoid(torch.matmul(sampled_hidden, self.W.t()) + self.visible_bias)\n",
    "        return p_visible_given_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j and retrieve knowledge graph vectors\n",
    "class Neo4jDataLoader:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def get_vectors(self):\n",
    "        with self._driver.session() as session:\n",
    "            # query = \"MATCH (node:Entity) RETURN node.vector AS vector\"\n",
    "            query = \"MATCH (n) RETURN n.Vector AS vector\"\n",
    "            result = session.run(query)\n",
    "            vectors = [record['vector'] for record in result]\n",
    "        return torch.tensor(vectors)\n",
    "neo4j_loader = Neo4jDataLoader(uri=\"bolt://34.101.159.175:7687\", user=\"neo4j\", password=\"unej1234\")\n",
    "data = neo4j_loader.get_vectors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://34.101.159.175:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"unej1234\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"MATCH (n) RETURN n.label AS label\")\n",
    "    label = pd.DataFrame([record.values() for record in result], columns=result.keys())\n",
    "    \n",
    "# Extract the values from the 'label' column and convert to a one-dimensional list\n",
    "label_values = label['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4801,  0.6460, -0.9932,  ..., -0.4361, -0.8677, -1.7822],\n",
       "        [ 0.2617,  0.0716,  0.0499,  ..., -1.2279, -0.1212, -1.0645],\n",
       "        [ 0.0228,  0.3414, -0.4033,  ..., -0.0076, -0.9350, -1.0907],\n",
       "        ...,\n",
       "        [-0.4614,  0.1860,  0.3704,  ..., -1.3313, -0.4901, -0.2599],\n",
       "        [-0.2461,  0.1481, -0.3852,  ..., -0.1658, -0.8010, -1.0049],\n",
       "        [ 1.8812,  0.1237, -2.4698,  ..., -1.5062, -2.7816,  0.4476]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [0.78980684]\n",
      "1         [1.4231793]\n",
      "2         [0.9084352]\n",
      "3       [-0.38928694]\n",
      "4       [-0.74302006]\n",
      "            ...      \n",
      "100       [0.6022621]\n",
      "101      [0.13106883]\n",
      "102      [-1.4819928]\n",
      "103      [0.06058532]\n",
      "104    [-0.056747735]\n",
      "Name: embeddings, Length: 105, dtype: object\n",
      "tensor([[ 0.7898],\n",
      "        [ 1.4232],\n",
      "        [ 0.9084],\n",
      "        [-0.3893],\n",
      "        [-0.7430],\n",
      "        [ 0.4056],\n",
      "        [-0.4913],\n",
      "        [-0.6787],\n",
      "        [-0.5073],\n",
      "        [ 0.9615],\n",
      "        [ 0.9742],\n",
      "        [-2.0069],\n",
      "        [-0.1508],\n",
      "        [ 0.1580],\n",
      "        [-0.7407],\n",
      "        [ 0.5534],\n",
      "        [-0.2743],\n",
      "        [ 0.2260],\n",
      "        [-0.5307],\n",
      "        [ 1.1660],\n",
      "        [ 0.8677],\n",
      "        [ 0.5838],\n",
      "        [-0.7661],\n",
      "        [ 0.4820],\n",
      "        [-0.8932],\n",
      "        [-0.1494],\n",
      "        [ 0.6777],\n",
      "        [-0.3936],\n",
      "        [-0.1938],\n",
      "        [ 0.5743],\n",
      "        [ 0.0045],\n",
      "        [ 1.0079],\n",
      "        [-0.9604],\n",
      "        [ 0.5007],\n",
      "        [-0.8760],\n",
      "        [-0.7847],\n",
      "        [-0.9132],\n",
      "        [-1.1953],\n",
      "        [ 0.8844],\n",
      "        [ 0.8826],\n",
      "        [-0.1982],\n",
      "        [-0.0819],\n",
      "        [ 1.0530],\n",
      "        [ 0.0608],\n",
      "        [-0.2840],\n",
      "        [-0.0410],\n",
      "        [-1.4808],\n",
      "        [-0.4054],\n",
      "        [-0.1146],\n",
      "        [ 0.3295],\n",
      "        [ 0.7105],\n",
      "        [ 1.0812],\n",
      "        [ 1.4352],\n",
      "        [-0.1861],\n",
      "        [-0.3789],\n",
      "        [-0.1089],\n",
      "        [-0.4642],\n",
      "        [-0.1467],\n",
      "        [ 1.3314],\n",
      "        [-0.8804],\n",
      "        [-0.1466],\n",
      "        [ 1.5235],\n",
      "        [-0.4957],\n",
      "        [-0.5696],\n",
      "        [ 0.0575],\n",
      "        [ 0.9793],\n",
      "        [ 0.1374],\n",
      "        [ 0.2787],\n",
      "        [ 0.6891],\n",
      "        [ 0.5568],\n",
      "        [ 0.9538],\n",
      "        [ 0.9273],\n",
      "        [ 0.2387],\n",
      "        [ 0.6047],\n",
      "        [ 0.0190],\n",
      "        [ 0.1705],\n",
      "        [-0.3910],\n",
      "        [ 0.6633],\n",
      "        [-0.2638],\n",
      "        [-0.2053],\n",
      "        [ 1.4022],\n",
      "        [ 0.8980],\n",
      "        [ 1.2415],\n",
      "        [ 1.3643],\n",
      "        [ 0.3272],\n",
      "        [ 0.1859],\n",
      "        [ 1.5061],\n",
      "        [-0.3341],\n",
      "        [-1.0521],\n",
      "        [-0.4798],\n",
      "        [-0.2872],\n",
      "        [ 0.6798],\n",
      "        [ 0.2213],\n",
      "        [ 0.3694],\n",
      "        [ 1.4684],\n",
      "        [-0.3246],\n",
      "        [-0.8917],\n",
      "        [-1.1652],\n",
      "        [-0.6061],\n",
      "        [ 0.0510],\n",
      "        [ 0.6023],\n",
      "        [ 0.1311],\n",
      "        [-1.4820],\n",
      "        [ 0.0606],\n",
      "        [-0.0567]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tq/8dxrnbkd5bx737ykn8j5w4j40000gn/T/ipykernel_2882/1662484291.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  embeddings_tensor = torch.tensor(df['embeddings'].to_list())\n"
     ]
    }
   ],
   "source": [
    "# Define a Word2Vec model (you need to define this as you did before)\n",
    "sentences = [str(text).split() for text in label_values]\n",
    "model = Word2Vec(sentences, vector_size=1, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Function to get embeddings for a list of words\n",
    "def get_sentence_embedding(word_list):\n",
    "    word_vectors = [model.wv[word] for word in word_list if word in model.wv.key_to_index]\n",
    "    \n",
    "    if word_vectors:\n",
    "        sentence_embedding = sum(word_vectors)\n",
    "        return sentence_embedding\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'text_data': label_values}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the text_data column into lists of words and apply the function to each row\n",
    "df['text_data'] = df['text_data'].apply(lambda x: x.split() if x is not None else [])\n",
    "df['embeddings'] = df['text_data'].apply(lambda x: get_sentence_embedding(x) if x else None)\n",
    "\n",
    "print(df['embeddings'])\n",
    "\n",
    "# # Filter out rows where embeddings are not available\n",
    "# df = df.dropna(subset=['embeddings'])\n",
    "\n",
    "# Replace rows where embeddings are not available with a default value (e.g., zeros)\n",
    "default_embedding = np.zeros(1)  # Replace with your desired default value\n",
    "df['embeddings'] = df['embeddings'].apply(lambda x: x if x is not None else default_embedding)\n",
    "\n",
    "# Convert embeddings to a PyTorch tensor\n",
    "embeddings_tensor = torch.tensor(df['embeddings'].to_list())\n",
    "\n",
    "print(embeddings_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x10 and 32x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/100], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     train_dbn()\n",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m, in \u001b[0;36mtrain_dbn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m     output \u001b[38;5;241m=\u001b[39m dbn(inputs)\n\u001b[1;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, targets)\n\u001b[1;32m     31\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mDBN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrbm1(x))\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrbm2(x))\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mRBM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     p_hidden_given_visible \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_bias)\n\u001b[1;32m     25\u001b[0m     sampled_hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(p_hidden_given_visible)\n\u001b[1;32m     26\u001b[0m     p_visible_given_hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mmatmul(sampled_hidden, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mt()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisible_bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x10 and 32x32)"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "def train_dbn():\n",
    "    # Initialize DBN and other hyperparameters\n",
    "    input_dim = 32  # Adjust based on the dimensionality of your knowledge graph vectors\n",
    "    hidden_dim = 32\n",
    "    output_dim = 2  # Adjust based on your task (e.g., classification)\n",
    "\n",
    "    dbn = DBN(input_dim, hidden_dim, output_dim)\n",
    "    optimizer = optim.Adam(dbn.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Connect to Neo4j and retrieve data\n",
    "    neo4j_loader = Neo4jDataLoader(uri=\"bolt://34.101.159.175:7687\", user=\"neo4j\", password=\"unej1234\")\n",
    "    data = neo4j_loader.get_vectors()\n",
    "\n",
    "    # Load labels for your data\n",
    "    labels = embeddings_tensor\n",
    "\n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    batch_size = 32  # Adjust based on your dataset size\n",
    "    dataset = TensorDataset(data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):  # Adjust the number of epochs\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = dbn(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dbn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
